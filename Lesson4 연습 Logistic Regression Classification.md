# Lession4 연습: Logistic Regression Classification

##  로지스틱 회귀 분류 모델을 사용한 기계 학습 연습

- code(불완전한 소스코드)

```python
import tensorflow as tf

x_data = [[1, 2], [2, 3], [3, 1], [4, 3], [5, 3], [6, 2]]
y_data = [[0], [0], [0], [1], [1], [1]]

X = tf.Variable(x_data, dtype=tf.float32, shape=[None, 2])
Y = tf.Variable(y_data, dtype=tf.float32, shape=[None, 1])

W = tf.Variable(tf.random.normal([2, 1]), dtype=tf.float32, name='weight', shape=[2, 1])
b = tf.Variable(tf.random.normal([1]), dtype=tf.float32, name='bias', shape=[1])

@tf.function
def hypothesis():
    return tf.sigmoid(tf.matmul(X, W)) + b

@tf.function
def cost(ht):
    return -tf.reduce_mean(Y*tf.math.log(ht) + (1-Y)*tf.math.log(1-ht))

optimizer = tf.keras.optimizers.SGD(learning_rate=0.01)
trainable_vars = [W, b]
print(f"trainable_vars={trainable_vars}")

@tf.function
def predict(ht):
    return tf.cast(ht > 0.5, dtype=tf.float32)

@tf.function
def accuracy(pt):
    return tf.reduce_mean(tf.cast(tf.equal(pt, Y), dtype=tf.float32))

for step in range(10001):
    with tf.GradientTape() as tp:
        ht_val = hypothesis()
        cost_val = cost(abs(ht_val))

    train_val = optimizer.minimize(cost_val, var_list=[W, b], tape=tp)
    if step % 2000 == 0:
        print(f"STEP = {step:06}, cost 함수값 = {cost_val}, 가설의 값 = {ht_val}")

h = hypothesis()
c = predict(h)
a = accuracy(c)
print(f"가설식의 값 = {h},\n실제의 값 = {c},\n정확도 = {a}")

```

- 결과

```cmd
STEP = 000000, cost 함수값 = 1.0390934944152832, 가설의 값 = [[ 0.31187814]
 [ 0.34871638]
 [-0.36142012]
 [-0.05190453]
 [-0.2733121 ]
 [-0.48289192]]
2021-12-16 15:26:32.945349: I tensorflow/stream_executor/cuda/cuda_blas.cc:1774] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.
STEP = 002000, cost 함수값 = 0.3888532221317291, 가설의 값 = [[ 0.00145769]
 [-0.03580058]
 [-0.726182  ]
 [-0.65665704]
 [-0.73727816]
 [-0.7599303 ]]
STEP = 004000, cost 함수값 = 0.3881579637527466, 가설의 값 = [[ 0.00239968]
 [-0.03896564]
 [-0.73931307]
 [-0.6779866 ]
 [-0.7493419 ]
 [-0.76706994]]
STEP = 006000, cost 함수값 = 0.38804951310157776, 가설의 값 = [[ 0.00236762]
 [-0.04077673]
 [-0.74381816]
 [-0.68560284]
 [-0.7534686 ]
 [-0.76958597]]
STEP = 008000, cost 함수값 = 0.38802552223205566, 가설의 값 = [[ 0.00211662]
 [-0.04179895]
 [-0.74574107]
 [-0.6887929 ]
 [-0.75524247]
 [-0.7707405 ]]
STEP = 010000, cost 함수값 = 0.38800621032714844, 가설의 값 = [[ 0.00079185]
 [-0.04358107]
 [-0.7475753 ]
 [-0.6912629 ]
 [-0.7570264 ]
 [-0.77224666]]
WARNING:tensorflow:AutoGraph could not transform <function predict at 0x000002CDE3E779D8> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.
Cause: 'arguments' object has no attribute 'posonlyargs'
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
WARNING:tensorflow:AutoGraph could not transform <function accuracy at 0x000002CDE3E77948> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.
Cause: 'arguments' object has no attribute 'posonlyargs'
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
가설식의 값 = [[ 6.8175793e-04]
 [-4.3704391e-02]
 [-7.4767125e-01]
 [-6.9137061e-01]
 [-7.5712317e-01]
 [-7.7234036e-01]],
실제의 값 = [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]],
정확도 = 0.5

Process finished with exit code 0

```

## 로지스틱 회귀 분류 모델을 실제 데이터에 적용

- code

```python
import tensorflow as tf
import numpy as np

xy = np.loadtxt('data-03-diabetes.csv', delimiter=',', dtype=np.float32)
x_data = xy[:, 0:-1]
y_data = xy[:, [-1]]

X = tf.Variable(x_data, dtype=tf.float32, shape=[None, 8])
Y = tf.Variable(y_data, dtype=tf.float32, shape=[None, 1])

W = tf.Variable(tf.random.normal([8, 1]), name='weight')
b = tf.Variable(tf.random.normal([1]), name='bias')

@tf.function
def hypo():
    return tf.sigmoid(tf.matmul(X, W) + b)

@tf.function
def cost(hypo):
    return -tf.reduce_mean(Y*tf.math.log(hypo) + (1-Y) * tf.math.log(1-hypo))

@tf.function
def predict(hypo):
    return tf.cast(hypo > 0.5, dtype=tf.float32)

@tf.function
def accuracy(pd):
    return tf.reduce_mean(tf.cast(tf.equal(pd, Y), dtype=tf.float32))

optimizer = tf.keras.optimizers.SGD(learning_rate=0.01)

for step in range(10001):
    with tf.GradientTape() as tp:
        ht_val = hypo()
        cost_val = cost(abs(ht_val))

    train_val = optimizer.minimize(cost_val, var_list=[W, b], tape=tp)
    if step % 2000 == 0:
        print(f"STEP={step:06}, cost 함수값 = {cost_val}")

h = hypo()
p = predict(h)
a = accuracy(p)
print(f"\n 가설식의 값 = {h},\n\n실제의 값={p},\n\n정확도={a}")
```

- 결과

```cmd
D:\engine\venv\Scripts\python.exe D:/Python_Projects/SampleProject1/exercisetf9.py
2021-12-16 16:53:17.205233: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX AVX2
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2021-12-16 16:53:17.557450: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5468 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3070, pci bus id: 0000:01:00.0, compute capability: 8.6
WARNING:tensorflow:AutoGraph could not transform <function hypo at 0x00000161F700D678> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.
Cause: 'arguments' object has no attribute 'posonlyargs'
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
WARNING:tensorflow:AutoGraph could not transform <function cost at 0x00000161F7004DC8> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.
Cause: 'arguments' object has no attribute 'posonlyargs'
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
STEP=000000, cost 함수값 = 1.2819491624832153
2021-12-16 16:53:18.421679: I tensorflow/stream_executor/cuda/cuda_blas.cc:1774] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.
STEP=002000, cost 함수값 = 0.5731958746910095
STEP=004000, cost 함수값 = 0.5193700194358826
STEP=006000, cost 함수값 = 0.4984075427055359
STEP=008000, cost 함수값 = 0.48832499980926514
STEP=010000, cost 함수값 = 0.4827880859375
WARNING:tensorflow:AutoGraph could not transform <function predict at 0x00000161F70163A8> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.
Cause: 'arguments' object has no attribute 'posonlyargs'
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
WARNING:tensorflow:AutoGraph could not transform <function accuracy at 0x00000161BD96B288> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.
Cause: 'arguments' object has no attribute 'posonlyargs'
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

 가설식의 값 = [[0.42722243]
 [0.9226312 ]
 [0.36316678]
 [0.9329167 ]
 [0.35304943]
 [0.79390305]
 [0.94137347]
 [0.6079877 ]
 [0.2510459 ]
 [0.50464493]
 [0.639349  ]
 [0.19439769]
 [0.44935733]
 [0.25185794]
 [0.7970516 ]
 [0.40843803]
 [0.7567809 ]
 [0.83164114]
 [0.80122465]
 [0.55988026]
 [0.6646631 ]
 [0.09793175]
 [0.6535201 ]
 [0.6630844 ]
 [0.33463624]
 [0.9373909 ]
 [0.58809096]
 [0.6209425 ]
 [0.65409595]
 [0.49636334]
 [0.94811857]
 [0.89552355]
 [0.6202961 ]
 [0.8914665 ]
 [0.41394013]
 [0.6729036 ]
 [0.8026816 ]
 [0.6624893 ]
 [0.42106524]
 [0.3700201 ]
 [0.80657995]
 [0.10102455]
 [0.4872204 ]
 [0.10660534]
 [0.6603994 ]
 [0.93128514]
 [0.65305525]
 [0.7400291 ]
 [0.93920445]
 [0.941865  ]
 [0.93651676]
 [0.22460864]
 [0.4124796 ]
 [0.96031314]
 [0.1930996 ]
 [0.51186347]
 [0.21747853]
 [0.6668341 ]
 [0.8751421 ]
 [0.52863246]
 [0.96613574]
 [0.7890544 ]
 [0.6836624 ]
 [0.82674086]
 [0.6122687 ]
 [0.546276  ]
 [0.9592269 ]
 [0.65805966]
 [0.8599713 ]
 [0.7103781 ]
 [0.25057143]
 [0.6165417 ]
 [0.89890707]
 [0.92203015]
 [0.8853683 ]
 [0.7413664 ]
 [0.38863656]
 [0.8591271 ]
 [0.8951879 ]
 [0.9121518 ]
 [0.8841609 ]
 [0.8379488 ]
 [0.22966154]
 [0.8181146 ]
 [0.54729164]
 [0.825966  ]
 [0.3938665 ]
 [0.8734695 ]
 [0.9616352 ]
 [0.7197069 ]
 [0.7301018 ]
 [0.7296906 ]
 [0.7487727 ]
 [0.53241885]
 [0.87731063]
 [0.98009104]
 [0.9088856 ]
 [0.464114  ]
 [0.31593364]
 [0.70104384]
 [0.68037796]
 [0.95250016]
 [0.79725015]
 [0.81337553]
 [0.87741107]
 [0.6982546 ]
 [0.916178  ]
 [0.75812775]
 [0.46099612]
 [0.3485747 ]
 [0.9203848 ]
 [0.88160056]
 [0.5045045 ]
 [0.4715524 ]
 [0.6263922 ]
 [0.8776568 ]
 [0.86536646]
 [0.91693306]
 [0.13162093]
 [0.7107136 ]
 [0.8332588 ]
 [0.6626683 ]
 [0.64273643]
 [0.7515645 ]
 [0.62090397]
 [0.83368224]
 [0.7543262 ]
 [0.7097705 ]
 [0.4704281 ]
 [0.56900895]
 [0.42809775]
 [0.7302643 ]
 [0.94460607]
 [0.77799684]
 [0.8101609 ]
 [0.87312496]
 [0.53653634]
 [0.7306058 ]
 [0.7991912 ]
 [0.6274888 ]
 [0.8745217 ]
 [0.6718469 ]
 [0.5450174 ]
 [0.67569077]
 [0.88634217]
 [0.84123427]
 [0.4518837 ]
 [0.9054693 ]
 [0.62682766]
 [0.85314596]
 [0.32186836]
 [0.34700385]
 [0.07828755]
 [0.15612629]
 [0.92784834]
 [0.9056356 ]
 [0.92711776]
 [0.1442217 ]
 [0.49938592]
 [0.74548864]
 [0.48182976]
 [0.85048515]
 [0.49197906]
 [0.8087972 ]
 [0.5606638 ]
 [0.72484076]
 [0.76250845]
 [0.83404386]
 [0.76710665]
 [0.5955413 ]
 [0.89540654]
 [0.868184  ]
 [0.9469012 ]
 [0.29311022]
 [0.8320572 ]
 [0.09621169]
 [0.30449924]
 [0.4367096 ]
 [0.89887345]
 [0.6685134 ]
 [0.9126929 ]
 [0.92811614]
 [0.6401711 ]
 [0.14860488]
 [0.2374806 ]
 [0.6904703 ]
 [0.7959572 ]
 [0.6270989 ]
 [0.87370276]
 [0.658979  ]
 [0.4273996 ]
 [0.18083908]
 [0.9003427 ]
 [0.34139818]
 [0.89722204]
 [0.91626   ]
 [0.783177  ]
 [0.61355156]
 [0.621123  ]
 [0.4934559 ]
 [0.74610776]
 [0.94711405]
 [0.7233271 ]
 [0.82932407]
 [0.12826882]
 [0.25203896]
 [0.8893122 ]
 [0.1805646 ]
 [0.9253884 ]
 [0.24702391]
 [0.19805863]
 [0.40282252]
 [0.6691833 ]
 [0.22517115]
 [0.7709966 ]
 [0.74121016]
 [0.8455081 ]
 [0.6538741 ]
 [0.19756652]
 [0.4105023 ]
 [0.77614015]
 [0.6149426 ]
 [0.9334047 ]
 [0.9065637 ]
 [0.68016094]
 [0.39535615]
 [0.08500414]
 [0.516683  ]
 [0.31635997]
 [0.32828316]
 [0.9523833 ]
 [0.6507559 ]
 [0.94499433]
 [0.204273  ]
 [0.10030581]
 [0.29069158]
 [0.8576216 ]
 [0.8946531 ]
 [0.876766  ]
 [0.7422819 ]
 [0.7904104 ]
 [0.5264311 ]
 [0.17926683]
 [0.600647  ]
 [0.06766575]
 [0.53863907]
 [0.80228025]
 [0.74044365]
 [0.72285897]
 [0.93938   ]
 [0.7728557 ]
 [0.80083996]
 [0.7599395 ]
 [0.80559564]
 [0.8208955 ]
 [0.44121662]
 [0.36956462]
 [0.6289202 ]
 [0.8171616 ]
 [0.545365  ]
 [0.7088328 ]
 [0.8191436 ]
 [0.42267913]
 [0.5384885 ]
 [0.80166525]
 [0.6833081 ]
 [0.39792517]
 [0.91168094]
 [0.81030524]
 [0.9333749 ]
 [0.58482414]
 [0.79145455]
 [0.819369  ]
 [0.83797956]
 [0.77799773]
 [0.87750673]
 [0.35531852]
 [0.53786504]
 [0.5946032 ]
 [0.37260523]
 [0.8289661 ]
 [0.27390307]
 [0.5387353 ]
 [0.9472504 ]
 [0.7723487 ]
 [0.81606287]
 [0.7181003 ]
 [0.50294214]
 [0.6758568 ]
 [0.5607349 ]
 [0.49449632]
 [0.6661101 ]
 [0.62611777]
 [0.6048259 ]
 [0.7600127 ]
 [0.3087332 ]
 [0.7020835 ]
 [0.8849739 ]
 [0.3150168 ]
 [0.70547163]
 [0.6969181 ]
 [0.51314425]
 [0.708062  ]
 [0.6140883 ]
 [0.7326254 ]
 [0.9023195 ]
 [0.656075  ]
 [0.68752986]
 [0.8641002 ]
 [0.5843169 ]
 [0.82016623]
 [0.93939495]
 [0.3424793 ]
 [0.745913  ]
 [0.31174064]
 [0.77196866]
 [0.7700655 ]
 [0.675673  ]
 [0.4449197 ]
 [0.71826965]
 [0.7117689 ]
 [0.7552729 ]
 [0.1971769 ]
 [0.696852  ]
 [0.83820975]
 [0.71180457]
 [0.9224846 ]
 [0.22718623]
 [0.7533588 ]
 [0.9487257 ]
 [0.16008873]
 [0.59471226]
 [0.7094609 ]
 [0.38329124]
 [0.16074495]
 [0.8358066 ]
 [0.9233211 ]
 [0.84574175]
 [0.5985396 ]
 [0.6838832 ]
 [0.51670676]
 [0.8032548 ]
 [0.85898226]
 [0.93955433]
 [0.6457046 ]
 [0.69986904]
 [0.5881566 ]
 [0.9044612 ]
 [0.93920374]
 [0.6970074 ]
 [0.29996902]
 [0.77554226]
 [0.34363705]
 [0.7675125 ]
 [0.22459157]
 [0.28941646]
 [0.46759096]
 [0.5727649 ]
 [0.37766922]
 [0.51002854]
 [0.8497362 ]
 [0.7097213 ]
 [0.8614117 ]
 [0.93123263]
 [0.66885006]
 [0.1817592 ]
 [0.6264889 ]
 [0.8300152 ]
 [0.8550134 ]
 [0.7430828 ]
 [0.28162882]
 [0.86826164]
 [0.8930841 ]
 [0.21886356]
 [0.59267807]
 [0.86119264]
 [0.86155635]
 [0.8863914 ]
 [0.93713915]
 [0.87232536]
 [0.92106825]
 [0.6802967 ]
 [0.46399915]
 [0.5217064 ]
 [0.85781825]
 [0.8701417 ]
 [0.18820098]
 [0.8030166 ]
 [0.8803985 ]
 [0.40013316]
 [0.7599363 ]
 [0.92010283]
 [0.53747076]
 [0.94011873]
 [0.23400937]
 [0.8393326 ]
 [0.59798723]
 [0.84756154]
 [0.38100228]
 [0.6740167 ]
 [0.76158804]
 [0.8620728 ]
 [0.20378941]
 [0.22135563]
 [0.661897  ]
 [0.8015993 ]
 [0.40164134]
 [0.77594644]
 [0.534063  ]
 [0.29035503]
 [0.87421435]
 [0.39728883]
 [0.9547941 ]
 [0.82515776]
 [0.57265145]
 [0.9221232 ]
 [0.6815066 ]
 [0.77602154]
 [0.2803496 ]
 [0.24917412]
 [0.7741955 ]
 [0.41063967]
 [0.4035105 ]
 [0.8499067 ]
 [0.92109525]
 [0.898743  ]
 [0.9364881 ]
 [0.7247684 ]
 [0.9064389 ]
 [0.34079146]
 [0.30128682]
 [0.53687626]
 [0.9370621 ]
 [0.612853  ]
 [0.12996824]
 [0.92223024]
 [0.78917974]
 [0.6969931 ]
 [0.7663611 ]
 [0.04215379]
 [0.9199748 ]
 [0.76271236]
 [0.78334296]
 [0.73384047]
 [0.9657133 ]
 [0.70011127]
 [0.73312116]
 [0.85833687]
 [0.841609  ]
 [0.19161095]
 [0.74636245]
 [0.9067012 ]
 [0.6769577 ]
 [0.82069826]
 [0.96205986]
 [0.86828864]
 [0.8691544 ]
 [0.6557308 ]
 [0.8239264 ]
 [0.93764675]
 [0.7648799 ]
 [0.6904237 ]
 [0.26947263]
 [0.41067722]
 [0.5078982 ]
 [0.5356791 ]
 [0.5331149 ]
 [0.7995176 ]
 [0.54921335]
 [0.8030768 ]
 [0.83737606]
 [0.76824814]
 [0.6390416 ]
 [0.44458607]
 [0.5168625 ]
 [0.9356663 ]
 [0.78887314]
 [0.26891327]
 [0.40433043]
 [0.47666   ]
 [0.10883566]
 [0.86875427]
 [0.21828516]
 [0.8914217 ]
 [0.82542723]
 [0.8095689 ]
 [0.7452372 ]
 [0.8889835 ]
 [0.43240684]
 [0.8206346 ]
 [0.92862123]
 [0.35112286]
 [0.5168683 ]
 [0.85224766]
 [0.8275596 ]
 [0.66354334]
 [0.82876325]
 [0.7602009 ]
 [0.80226463]
 [0.22278278]
 [0.79186416]
 [0.92518294]
 [0.703716  ]
 [0.78888595]
 [0.7301074 ]
 [0.85683995]
 [0.90109813]
 [0.9293476 ]
 [0.5513462 ]
 [0.46835506]
 [0.8016626 ]
 [0.7541334 ]
 [0.9674916 ]
 [0.73475564]
 [0.7120351 ]
 [0.48451933]
 [0.7368614 ]
 [0.929967  ]
 [0.9574513 ]
 [0.8651774 ]
 [0.7256237 ]
 [0.77739906]
 [0.82192224]
 [0.45197123]
 [0.8455484 ]
 [0.84612495]
 [0.9274057 ]
 [0.6469687 ]
 [0.7079208 ]
 [0.9455562 ]
 [0.51903284]
 [0.58426285]
 [0.66044086]
 [0.71436465]
 [0.6620064 ]
 [0.83205384]
 [0.90866685]
 [0.1953423 ]
 [0.13375774]
 [0.7030799 ]
 [0.5269671 ]
 [0.22472157]
 [0.8216082 ]
 [0.9066501 ]
 [0.7162205 ]
 [0.9334795 ]
 [0.89400995]
 [0.77657896]
 [0.7926464 ]
 [0.7339523 ]
 [0.45408976]
 [0.7945264 ]
 [0.5952736 ]
 [0.11176473]
 [0.8842566 ]
 [0.8927967 ]
 [0.7539129 ]
 [0.9303662 ]
 [0.83116305]
 [0.87111443]
 [0.57627684]
 [0.66481173]
 [0.8862385 ]
 [0.75666696]
 [0.8729134 ]
 [0.8744826 ]
 [0.61407495]
 [0.83133554]
 [0.88784236]
 [0.5225689 ]
 [0.603866  ]
 [0.20267867]
 [0.21550687]
 [0.8379888 ]
 [0.610533  ]
 [0.63915056]
 [0.56906796]
 [0.9343557 ]
 [0.44794568]
 [0.85589755]
 [0.32033014]
 [0.93431574]
 [0.31053424]
 [0.73761255]
 [0.57816255]
 [0.9263731 ]
 [0.6503099 ]
 [0.22557575]
 [0.7209904 ]
 [0.95497704]
 [0.3423139 ]
 [0.9368462 ]
 [0.83333987]
 [0.8888607 ]
 [0.7895608 ]
 [0.42889807]
 [0.38456362]
 [0.67837507]
 [0.22769418]
 [0.9626069 ]
 [0.28086   ]
 [0.91234064]
 [0.8699051 ]
 [0.43460232]
 [0.21561396]
 [0.7048601 ]
 [0.42909613]
 [0.85718   ]
 [0.6934317 ]
 [0.9794133 ]
 [0.63754845]
 [0.66066444]
 [0.7259912 ]
 [0.87731695]
 [0.10459581]
 [0.6694943 ]
 [0.8169633 ]
 [0.8219487 ]
 [0.70908576]
 [0.5043397 ]
 [0.57615626]
 [0.8941041 ]
 [0.746191  ]
 [0.7626733 ]
 [0.85885805]
 [0.80938166]
 [0.8453974 ]
 [0.63950104]
 [0.8043141 ]
 [0.88711524]
 [0.69454277]
 [0.95155096]
 [0.789084  ]
 [0.6280396 ]
 [0.51248556]
 [0.8800751 ]
 [0.87340003]
 [0.37900662]
 [0.6672149 ]
 [0.26963612]
 [0.58059186]
 [0.80619353]
 [0.9502712 ]
 [0.819186  ]
 [0.72020733]
 [0.82385355]
 [0.8504578 ]
 [0.425593  ]
 [0.94437134]
 [0.6381002 ]
 [0.87565094]
 [0.35390866]
 [0.11174716]
 [0.22475499]
 [0.38322183]
 [0.6958611 ]
 [0.8047734 ]
 [0.49186546]
 [0.72609955]
 [0.8176648 ]
 [0.48113775]
 [0.39618796]
 [0.89690614]
 [0.8313446 ]
 [0.25039524]
 [0.6927389 ]
 [0.22531033]
 [0.45415804]
 [0.7584988 ]
 [0.7091318 ]
 [0.91302294]
 [0.9803475 ]
 [0.12864885]
 [0.6776822 ]
 [0.64195657]
 [0.49137715]
 [0.70520234]
 [0.80202943]
 [0.8772658 ]
 [0.78110164]
 [0.40068346]
 [0.7679316 ]
 [0.11410571]
 [0.6845955 ]
 [0.559552  ]
 [0.93051404]
 [0.4764981 ]
 [0.5232783 ]
 [0.84633213]
 [0.64164984]
 [0.48259726]
 [0.70699537]
 [0.6700653 ]
 [0.3186289 ]
 [0.6004728 ]
 [0.8543653 ]
 [0.76765925]
 [0.62930435]
 [0.7457131 ]
 [0.3008032 ]
 [0.83823365]
 [0.5275724 ]
 [0.7561955 ]
 [0.38673997]
 [0.6772645 ]
 [0.8358936 ]
 [0.23092085]
 [0.36309347]
 [0.82853454]
 [0.8283343 ]
 [0.73892725]
 [0.87031764]
 [0.75102186]
 [0.70005107]
 [0.6395704 ]
 [0.7404242 ]
 [0.622112  ]
 [0.7525475 ]
 [0.44479063]
 [0.5089367 ]
 [0.89801157]
 [0.7766755 ]
 [0.6661008 ]
 [0.23314382]
 [0.8869658 ]
 [0.8693452 ]
 [0.80712825]
 [0.6830421 ]
 [0.8733109 ]
 [0.8320188 ]
 [0.7349127 ]
 [0.41376367]
 [0.896282  ]
 [0.8983553 ]
 [0.3813627 ]
 [0.19478047]
 [0.74339086]
 [0.25699642]
 [0.7963074 ]
 [0.28038508]
 [0.54671943]
 [0.5799289 ]
 [0.7196376 ]
 [0.8608727 ]
 [0.10263566]
 [0.36930966]
 [0.64050967]
 [0.48575726]
 [0.5188638 ]
 [0.7495296 ]
 [0.1520419 ]
 [0.9345798 ]
 [0.14068422]
 [0.89092624]
 [0.73482   ]
 [0.6577961 ]
 [0.82259965]
 [0.75825274]
 [0.8791936 ]],

실제의 값=[[0.]
 [1.]
 [0.]
 [1.]
 [0.]
 [1.]
 [1.]
 [1.]
 [0.]
 [1.]
 [1.]
 [0.]
 [0.]
 [0.]
 [1.]
 [0.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [0.]
 [1.]
 [1.]
 [0.]
 [1.]
 [1.]
 [1.]
 [1.]
 [0.]
 [1.]
 [1.]
 [1.]
 [1.]
 [0.]
 [1.]
 [1.]
 [1.]
 [0.]
 [0.]
 [1.]
 [0.]
 [0.]
 [0.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [0.]
 [0.]
 [1.]
 [0.]
 [1.]
 [0.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [0.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [0.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [0.]
 [1.]
 [1.]
 [1.]
 [0.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [0.]
 [0.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [0.]
 [0.]
 [1.]
 [1.]
 [1.]
 [0.]
 [1.]
 [1.]
 [1.]
 [1.]
 [0.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [0.]
 [1.]
 [0.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [0.]
 [1.]
 [1.]
 [1.]
 [0.]
 [0.]
 [0.]
 [0.]
 [1.]
 [1.]
 [1.]
 [0.]
 [0.]
 [1.]
 [0.]
 [1.]
 [0.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [0.]
 [1.]
 [0.]
 [0.]
 [0.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [0.]
 [0.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [0.]
 [0.]
 [1.]
 [0.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [0.]
 [1.]
 [1.]
 [1.]
 [1.]
 [0.]
 [0.]
 [1.]
 [0.]
 [1.]
 [0.]
 [0.]
 [0.]
 [1.]
 [0.]
 [1.]
 [1.]
 [1.]
 [1.]
 [0.]
 [0.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [0.]
 [0.]
 [1.]
 [0.]
 [0.]
 [1.]
 [1.]
 [1.]
 [0.]
 [0.]
 [0.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [0.]
 [1.]
 [0.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [0.]
 [0.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [0.]
 [1.]
 [1.]
 [1.]
 [0.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [0.]
 [1.]
 [1.]
 [0.]
 [1.]
 [0.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [0.]
 [1.]
 [1.]
 [1.]
 [1.]
 [0.]
 [1.]
 [1.]
 [0.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [0.]
 [1.]
 [0.]
 [1.]
 [1.]
 [1.]
 [0.]
 [1.]
 [1.]
 [1.]
 [0.]
 [1.]
 [1.]
 [1.]
 [1.]
 [0.]
 [1.]
 [1.]
 [0.]
 [1.]
 [1.]
 [0.]
 [0.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [0.]
 [1.]
 [0.]
 [1.]
 [0.]
 [0.]
 [0.]
 [1.]
 [0.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [0.]
 [1.]
 [1.]
 [1.]
 [1.]
 [0.]
 [1.]
 [1.]
 [0.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [0.]
 [1.]
 [1.]
 [1.]
 [0.]
 [1.]
 [1.]
 [0.]
 [1.]
 [1.]
 [1.]
 [1.]
 [0.]
 [1.]
 [1.]
 [1.]
 [0.]
 [1.]
 [1.]
 [1.]
 [0.]
 [0.]
 [1.]
 [1.]
 [0.]
 [1.]
 [1.]
 [0.]
 [1.]
 [0.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [0.]
 [0.]
 [1.]
 [0.]
 [0.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [0.]
 [0.]
 [1.]
 [1.]
 [1.]
 [0.]
 [1.]
 [1.]
 [1.]
 [1.]
 [0.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [0.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [0.]
 [0.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [0.]
 [1.]
 [1.]
 [1.]
 [0.]
 [0.]
 [0.]
 [0.]
 [1.]
 [0.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [0.]
 [1.]
 [1.]
 [0.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [0.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [0.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [0.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [0.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [0.]
 [0.]
 [1.]
 [1.]
 [0.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [0.]
 [1.]
 [1.]
 [0.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [0.]
 [0.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [0.]
 [1.]
 [0.]
 [1.]
 [0.]
 [1.]
 [1.]
 [1.]
 [1.]
 [0.]
 [1.]
 [1.]
 [0.]
 [1.]
 [1.]
 [1.]
 [1.]
 [0.]
 [0.]
 [1.]
 [0.]
 [1.]
 [0.]
 [1.]
 [1.]
 [0.]
 [0.]
 [1.]
 [0.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [0.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [0.]
 [1.]
 [0.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [0.]
 [1.]
 [1.]
 [1.]
 [0.]
 [0.]
 [0.]
 [0.]
 [1.]
 [1.]
 [0.]
 [1.]
 [1.]
 [0.]
 [0.]
 [1.]
 [1.]
 [0.]
 [1.]
 [0.]
 [0.]
 [1.]
 [1.]
 [1.]
 [1.]
 [0.]
 [1.]
 [1.]
 [0.]
 [1.]
 [1.]
 [1.]
 [1.]
 [0.]
 [1.]
 [0.]
 [1.]
 [1.]
 [1.]
 [0.]
 [1.]
 [1.]
 [1.]
 [0.]
 [1.]
 [1.]
 [0.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [0.]
 [1.]
 [1.]
 [1.]
 [0.]
 [1.]
 [1.]
 [0.]
 [0.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [0.]
 [1.]
 [1.]
 [1.]
 [1.]
 [0.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [0.]
 [1.]
 [1.]
 [0.]
 [0.]
 [1.]
 [0.]
 [1.]
 [0.]
 [1.]
 [1.]
 [1.]
 [1.]
 [0.]
 [0.]
 [1.]
 [0.]
 [1.]
 [1.]
 [0.]
 [1.]
 [0.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]],

정확도=0.7720685005187988
```

